<post>
  <author>Doglatine</author>
  <date>2020-04-16T23:58:24Z</date>
  <link>/r/TheMotte/comments/g0ck7p/culture_war_roundup_for_the_week_of_april_13_2020/fnmzy2w/</link>
  <title>The Changing Way Philosophers and Psychologists Approach Beliefs</title>
  <body>
    <div class="md">
      <p>I wanted to discuss a topic from academic philosophy with the community, specifically the changing way that some philosophers and psychologists are thinking about beliefs. The tl;dr is that I think there have been some major insights in the last few decades into what beliefs are actually 'for', and this has upshots for the way we understand human behaviour and also the limits of rationality. This has some relevance for CW, but may be of interest for those of us who identify as Rationalist or Rat-adjacent (and I was pleased to see from <a href="/u/TracingWoodgrains">u/TracingWoodgrains</a>' survey that most of still feel at least some connection that community). I'll also flag that this isn't quite my specialisation in analytic philosophy, and I'm aware I'm not the only philosopher here, so I welcome objections to the way I'm presenting the story or its conclusions.</p>
      <p>Quick programming note: you'll note that I spoke above about beliefs plural, rather than belief singular (and henceforth capitalised). This is because there's something of split in philosophy between mainstream epistemology and philosophy of psychology/philosophy of mind with regard to how they approach the topic of belief. Classic epistemology focuses on the notion of Belief as a special kind of epistemological state capable of grounding knowledge. This is the kind of Belief that's at issue when we ask, for example, whether we know if there's an external world, or if true justified belief automatically counts as knowledge. It's very concerned with normative issues and relies heavily on conceptual analysis and intuitions (that's not to knock it, though - these are big questions). By contrast, a lot of the time when philosophers of mind and philosophers of psychology talk about beliefs, they are interested in belief <em>qua</em> mental representations - things inside our head that guide our behaviour. There are big debates as to whether beliefs in this sense - i.e., discreet cognitive processes with representational content - even exist, but I'll set that aside for now.</p>
      <p>The issue I want to discuss, then, is what the function of beliefs in this latter sense is - in other words, what are beliefs <em>for.</em> The answer might seem pretty obvious: they're for getting an accurate model of the world. Acquire a bunch of true beliefs, plug in your desires, and you then have a creature that wants certain things and has a pretty good idea about how to get them. The late great Jerry Fodor put this well -</p>
      <blockquote>
        <p>It's generally not much use knowing how the world is unless you are able to act on what you know (*mere knowing* won't even get you tenure; you've got to *publish*). And it's generally not much use knowing how to act on one's belief that the world is so-and-so unless the world *is* so-and-so... But put the two together and you have rational actions predicated on true beliefs, which is likely to get you lots of children.</p>
      </blockquote>
      <p>You might wonder what the "function" talk above is about. While I think there's more to be said here about psychological functions, as Fodor's reference to children should make clear we're also thinking about this in evolutionary terms. So when we ask what beliefs are for, a big part of what we're asking is why nature gave us a brain capable of forming representations about the external world. And Fodor's answer is: so that we have veridical (that is, true) models so we can live long, prosper, and most importantly, have lots of kids.</p>
      <p>This kind of view - which I'll idiosyncratically refer to as the <em>alethic</em> view of belief - was never the only game in town (especially once we pan the camera away from philosophers of psychology to epistemologists proper), and even Fodor was aware of its complications, but it's fair to say it's been very influential in the last four decades or so of cognitive science, and perhaps especially in AI.</p>
      <p>One thing it's worth noting about the alethic view is that it's not committed to the idea that actual humans are models of rationality. Everyone knows humans make stupid mistakes in reasoning and inference sometimes. But for the alethic view, these amount to <em>deviations</em> from proper function for the human belief system. Like any machine, our belief fixation mechanisms aren't perfect: sometimes they glitch or screw up, and may even do so in systematic ways, especially in contexts outside those in which evolved (e.g., the modern informational environment). But insofar as these are bugs in the code, so to speak, there's hope that we might squash them.</p>
      <p>However, this alethic view of beliefs has some serious and perhaps fundamental problems, as has been widely noted for a long time. In short, it's not clear that we should expect evolution to have selected belief-fixation systems to operate with accuracy and veridicality as their sole functions. Let me give three of the main sorts of case in which actually having <em>false</em> beliefs might be adaptive.</p>
      <p>
        <strong>Case 1:</strong>
        <strong>unequal payoffs for positive and negative errors.</strong> Imagine there's a hazy shape on the horizon. It pretty much looks like a bush, but it also looks a <em>little</em> like a panther. Let's say that a purely rational agent operating probabilistically would assign 80% chance to it being a bush, and 20% to its being a panther. Now, this ideal reasoner might decide to run, just to be on the safe side. But humans don't typically think probabilistically - we're shockingly bad at it, especially in the heat of the moment, and frequently in effect we round probabilities to 1 or 0 and get on with things. Needless to say, a Pleistocene human with these limitations who still prioritised accuracy over pragmatics in situations involving ambiguous shapes that could be large predators would... well, not be around long enough to have many kids. Similar situations might involve possible disease, bad food, threatening conspecifics, and so on. So we might expect evolution to have equipped us with belief-fixation mechanisms that are - at least in some domains - <em>instrumentally</em> rational but <em>epistemically</em> irrational, leading to systematic deviation from a purely alethic function for belief. Strike one against the alethic model.</p>
      <p>
        <strong>Case 2: emotional management.</strong> Evolution isn't building belief fixation mechanisms in a vacuum; it's building them on top of a 4.5 billion year old biological template, and one with plenty of pre-existing hardware. One of the most important bits of hardware is the affective system, that gives us core emotions like fear, disgust, anger, and so on as well as a bunch of social emotions like jealousy, resentment, social anxiety, and so on. This has the result that sometimes having accurate beliefs will have severe consequences for our emotional well-being and ultimately for our reproductive success. Perhaps Schopenhauer was right and everything sucks and we'd be better off not existing, but any creature that took that particular black pill wouldn't have had many descendants. More subtly, there might be times when forming accurate beliefs would lead us to ineffectual despair, loss of drive, or social self-isolation. There might consequently be good reasons for evolution to select for creatures whose beliefs are responsive to considerations of <em>emotional self-protection</em>. This is essentially the big insight of Leon Festinger and <a href="https://en.wikipedia.org/wiki/Cognitive_dissonance">cognitive dissonance theory</a>. Like most big psychological theories (especially those from the 50s to the 80s) this is a big, messy, hopelessly overambitious framework, but the core insight that a lot of our reasoning is motivated by concerns of emotional self-protection is a critical one. It led to ideas like the <a href="https://en.wikipedia.org/wiki/Just-world_hypothesis">Just World Hypothesis</a> and <a href="https://en.wikipedia.org/wiki/Terror_management_theory">Terror Management Theory</a>, and is involved in at least half of the big cognitive biases out there. It's also been one of the most influential frameworks for me just in understanding my own reasoning. These days, motivated reasoning is all the rage in cognitive science, and it's common to talk of belief fixation mechanisms as a kind of "cognitive immune system" for promoting certain kinds of evolutionarily adaptive attitudes and behaviours rather than as a process purely directed at truth. Strike two for the alethic view.</p>
      <p>(continued in comments because the goshdarn character limit)</p>
    </div>
  </body>
</post>