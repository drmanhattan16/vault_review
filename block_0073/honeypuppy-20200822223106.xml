<post>
  <author>honeypuppy</author>
  <date>2020-08-22T22:31:06Z</date>
  <link>/r/TheMotte/comments/ib82ju/culture_war_roundup_for_the_week_of_august_17_2020/g2iycrq/</link>
  <title>What Makes a "Weak Man" Argument?</title>
  <body>
    <div class="md">
      <p>
        <strong>What makes a “weak man” argument?</strong>
      </p>
      <p>In <a href="https://slatestarcodex.com/2014/05/12/weak-men-are-superweapons/">Weak Man Are Superweapons</a>, Scott Alexander defines a weak man as 
“a terrible argument that only a few unrepresentative people hold, which was only <em>brought to prominence</em> so your side had something easy to defeat.” He gives the example of an atheist who criticises religion by invoking the Westboro Baptist Church. </p>
      <p>That’s a clear-cut example of a weak man. However, I think a fair definition of “weak-manning” would extend beyond these extreme outliers to somewhat more common situations. The question is, where to draw the line?  </p>
      <p>For example, imagine a well-read creationist who debates non-expert believers in evolution, for example, random adults with arts degrees. There’s a good chance that the creationist will “win” these debates, in the sense that the evolutionists will likely make a number of bad arguments and be stumped by many of the points the creationist puts across. But no-one should significantly lower their credence in the theory of evolution on the basis of these debates. As non-experts in a technical field, the evolutionists were weak opponents. </p>
      <p>For these middle-ground weak men, it could be the case that a <em>large majority</em> of people who believe a certain position could be considered to have “weak man” arguments for it, especially if it’s a politically salient topic that ultimately rests on quite technical foundations. </p>
      <p>At the furthest extreme, you could define weak-manning as attacking <em>any</em> argument other than the very strongest ones for a given position - the steel men. It could even be the case that <em>none</em> of the people who believe a certain position actually know the steel man for it. The best examples are <a href="https://slatestarcodex.com/2019/06/04/book-review-the-secret-of-our-success/">traditions that arose through cultural evolution</a>, which can be successful even though the practitioners’ explanations may be obviously contrived superstitions. </p>
      <p>The “fallacy fallacy” is a crux here. The conclusion of an argument is not necessarily wrong just because the argument may have been fallacious.</p>
      <p>Yet the implications of the extreme view may be that we simply abandon the use of argument and debate entirely, for we can never rule out the possibility that a strong argument exists that no-one has presented yet. That seems to be going too far.</p>
      <p>I think we can get somewhere by evaluating what the <em>purpose</em> of an argument is.</p>
      <p>If you’re trying to make a strong claim about the ground truth of an empirical matter, or about the merits of an entire political philosophy, then I think you should stick as closely as possible to attempting to refute the strongest arguments only. </p>
      <p>If you’re just debating online because you find debate fun, or you enjoy picking apart arguments, then I guess that’s okay, so long as you acknowledge it. Even if you enjoy mindlessly browsing subreddits which solely focus on cherry-picking the stupidest-seeming arguments made by random people in your outgroup and interpreting them uncharitably for laughs, then as long as you’re aware that’s what you’re doing, maybe that’s okay too. However, I think it’s very easy for people in these groups to (at least subconsciously) feel like they’ve “debunked” their entire outgroup, so I think you should be careful in these situations. (In hindsight, I feel like quite a lot of my own internet debating over the years was like this).</p>
      <p>If you are to criticise weak (albeit common) arguments and claim to have high-minded motives, I think it’s doable but it requires a lot of care, and it’s reasonable for others to be concerned about a possible “superweapon” effect. 
One plausible reason might be: you’re concerned that popular weak arguments may help support bad policies or bad behaviours, even if there may plausibly be some more reasonable actions that side could do. For example, rent control can be a fairly popular policy among left-wingers, though most economists think it’s a bad idea even if your objective is to help renters. Criticising bad arguments for rent control doesn’t need to mean you’re criticising the entirety of the left.  </p>
      <p>Another might be that you have ideals about “raising the quality of discourse”. I think that a lot of Scott Alexander’s articles, especially his mid-2010s posts criticising internet social justice, <a href="https://slatestarcodex.com/2017/03/24/guided-by-the-beauty-of-our-weapons/">were done with this idea in mind</a>. Most of what he criticised did not have particularly strong advocates - they were things like viral Tumblr posts, feminist blogs with names like “Bitchtopia”, or clickbaity articles from places like <em>Salon</em>. But to his credit, he usually acknowledged they weren’t the strongest arguments on their side, but he thought it was worthwhile criticising them anyway.</p>
      <p>However, the danger of the “superweapon” effect, as Scott describes it in the original weak man article, is that a series of “individually harmless but collectively dangerous statements” be utilised as a weapon by bad actors. He gives the example of a Jew in czarist Russia, who observes a bunch of negative stories going around about Jews, none of which seem technically incorrect, but eventually lead to discrimination against the Jew.</p>
      <p>I think this is a reasonable concern to have about ostensibly caveated weak-men, including Scott’s own posts. Probably the most notable example of a SSC post achieving this effect is <a href="https://slatestarcodex.com/2016/11/16/you-are-still-crying-wolf/">You Are Still Crying Wolf</a>, which explicitly spelled out how SA thought Trump was terrible, but then went on to try to refute some of the more extreme anti-Trump arguments. Nonetheless, it ended up getting tweeted out by Ann Coulter and a bunch of pro-Trump bots, causing SA to eventually take it down and then put it back up with the disclaimer you now see. I somewhat agree with <a href="https://www.reddit.com/r/slatestarcodex/comments/8vqgq1/you_are_still_crying_wolf_2016_is_back_up/e1rojqb/">this Reddit comment</a> that criticised the post for enabling this by ‘distort[ing] the media landscape at the time by presenting hard left takes on Trump as "the media"’, despite agreeing that the post was ‘correct on the object level of Trump and racism’.</p>
      <p>Overall, it’s a hard call to make on the merits of criticising marginal weak-men. There’s often a reasonable defense that you’re helping improve the quality of discourse in some way. On the other hand, you can never quite rule out the possibility you’re helping (even inadvertently) to build a “superweapon”.</p>
      <p>I think the best strategy in such cases is to ensure your writing is <em>super</em>-caveated. If you’re attacking an argument that you know isn’t the strongest it could be, point that out. Give examples of better arguments made by people on the same side, or come up with your own if necessary. This is more important when you’re a popular blogger like Scott Alexander, whose posts are more likely to go viral than a random <a href="/r/TheMotte">/r/TheMotte</a> poster.</p>
    </div>
  </body>
</post>