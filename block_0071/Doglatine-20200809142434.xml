<post>
  <author>Doglatine</author>
  <date>2020-08-09T14:24:34Z</date>
  <link>/r/TheMotte/comments/i2r8qo/culture_war_roundup_for_the_week_of_august_03_2020/g0w713q/</link>
  <title>Beyond a Utilitarian Defense of Free Speech</title>
  <body>
    <div class="md">
      <p>tl;dr - for a long time I've been dissatisfied with the broadly utilitarian defenses of free speech that seem to be the default here and other places. So I've  been meaning to write a detailed, well-researched, high-effort post arguing that free speech is a basic political good whose justification doesn't rest on its benign consequences. This isn't that post, sadly, but it should give you an idea of where I'm coming from, and I'll be interested to hear what others say.</p>
      <p>
        <em>Imagine one day while out ice-climbing with friends you plunge down a crevasse, and are quickly immersed in freezing water, losing consciousness almost instantly. Ten thousand years later, you are awoken by a benevolent AI. The year is now 12,000 AD and the planet earth and most of the Solar System are populated by a host of different superintelligent beings - some virtual, some embodied, some the product of millennia-old mind uploading and others created ex nihilo. Some traditional embodied humans are still around, but not many, and they tend to either be religious fanatics, anarcho-primitivists, or AIs in the equivalent of fancy dress.</em>
      </p>
      <p>
        <em>As you begin the daunting task of acculturating yourself to this new civilisation, you're given access to a mine of archives in the form of text, video, and holos. Early signs are encouraging: the world of 12,000AD is incredibly technologically advanced, prosperous, free, and egalitarian, while policies are decided upon along broadly utilitarian principles. Governance such as is required is conducted via mass electronic participatory democracy. However, one day in the archives you find a few references to an ideology called "25th century neo-Aristotelian materialism", or 25CNAM for short. When you try to follow up these investigations, you hit a brick wall: all relevant files have been blocked or deleted.</em>
      </p>
      <p>
        <em>You ask your nearest benevolent superintelligent friend about this, and they're happy to explain. "Ah yes, 25CNAM. Quite an interesting ideology but very toxic, not to mention utterly false. Still, very believable, especially for non-superintelligent individuals. After extensive debate and modeling, we decided to ban all archival access save for the relevant historians and researchers. Every time we simmed it, the negative consequences - bitterness, resentment, prejudice - were unavoidable, and the net harms produced easily outweighed any positive consequences."</em>
      </p>
      <p>
        <em>"But what about the harms associated with censorship?" you ask. "This is the first piece of censorship I've seen in your whole society, and to be frank, it slightly changes the way I feel about things here. It makes me wonder what else you're hiding from me, for example. And having censored 25CNAM, you'll surely find it easier or more tempting to censor the next toxic philosophy that comes along. What about these long-range distributed negative consequences?"</em>
      </p>
      <p>
        <em>The AI's silver-skinned avatar smiles tolerantly. "Of course, these are important factors we took into consideration. Indeed, we conducted several simming exercises extrapolating all negative consequences of this decision more than a hundred thousand years into the future. Of course, there's too much uncertainty to make really accurate predictions, but the shape of the moral landscape was clear: it was overwhelmingly likely that censoring people's access to 25CNAM would have net beneficial consequences, even factoring in all those distributed negative consequences."</em>
      </p>
      <p>How would you feel in this position? For my part, I don't think I'd be satisfied. I might <em>believe</em> the AI's claim that society was better off with the censorship of 25CNAM, at least if we're understanding "better off" in some kind of broadly utilitarian terms. But I would feel that something of moral importance in that society had been lost. I think I'd say something like the following:</p>
      <p>
        <em>"I realise you're much smarter than me, but you say that I'm a citizen with full rights in your society. Which means that you presumably recognise me as an autonomous person, with the ability and duty to form beliefs in accordance with the demands of my reason and conscience. By keeping this information from me - not in the name of privacy, or urgent national security, but 'for my own good' - you have ceased to treat me as a fully autonomous agent. You've assumed a position of epistemic authority over me, and are treating me, in effect, like a child or an animal. Now, maybe you think that your position of epistemic authority is well deserved - you're a smart AI and I'm a human, after all. But in that case, don't pretend like you're treating me as an equal."</em>
      </p>
      <p>This, in effect, encapsulates why I'm dissatisfied with utilitarian defenses of free speech. The point is not that there aren't great broadly utilitarian justifications for the freedom speech - <a href="https://en.wikipedia.org/wiki/On_Liberty">there certainly are</a>. The point is rather that at least certain forms of free speech seem to me to be bound up with our ability to autonomously exercise our understanding, and that to me seems like <em>an unqualified</em> good - one that doesn't need to lean on utilitarian justifications for support.</p>
      <p>To give a more timely and concrete example, imagine if we get a COVID vaccine soon and a substantial anti-vaxx movement develops. A huge swathe of people globally begin sharing demonstrably false information about the vaccine and its 'harms', leading to billions of people deciding to not to take the vaccine, and tens of millions consequently dying unnecessarily. Because we're unable to eliminate the disease, sporadic lockdowns continue, leading to further economic harms and consequent suffering and death. But thinktanks and planners globally have a simple suggestion: even if we can't make people take the vaccine, we can force private companies to stop the spread of this ridiculous propaganda. Clearly, this will be justified in the long run!</p>
      <p>For my part, I don't think we'd be <em>obviously</em> justified in implementing this kind of censorship just because of the net harms it would avoid. Now, I realise that a lot of people here will probably want to agree with me on <em>specifically utilitarian grounds</em> - censorship is a ratchet and will lead to tyranny, because by censoring the anti-vaxx speech we push it underground, or miss the opportunity to defeat it publicly via reason and ridicule, etc.. But those are all contingent empirical predictions about possible harms associated with censorship. To those people, I'd ask: what if you were wrong? What if this case of censorship doesn't turn out to be a ratchet? What if we can extirpate the relevant harmful speech fully rather than pushing it underground? What if we can't defeat it by reason and ridicule alone? And so on.</p>
      <p>For my part, I don't think our defense of free speech should be held hostage to these kinds of empirical considerations. It rests, or so it seems to me, on something deeper which is hard to articulate clearly, but which roughly amounts to the fact that we're all rational agents trying to make our way in the world according to our own norms and values. That's a big part of what it means to be a human rather than an animal: we might care about suffering in animals, but we don't recognise that they have a right to autonomy or face the same cursed blessing that we have of trying to make rational sense of the world. By contrast, any reasonable ethical or political philosophy for the governance of human being has to start from the principle that we're all on our own journeys, our own missions to make sense of the world, and none of us is exempt from the game or qualified to serve as its ultimate arbiter. As soon as you sacrifice someone's ability to make up their own mind in order to protect their interests, you're no longer treating them as a human, but more like an animal.</p>
      <p>That's why although I'm sometimes sympathetic to utilitarianism in particular contexts, it also strikes me as a kind of 'farmyard morality': you should give the pigs straw because it'll help them sleep better, let the chickens run free because they'll be healthier, slaughter the pigs humanely because it minimises suffering. Needless to say, we're not in this position with our fellow human beings: instead, we're dealing with fellow rational agents, autonomous beings capable of making reasons for themselves that might be very different from our own. That's not to say that we <em>ourselves</em> need to treat everyone as having totally equivalent epistemic status: there are people we trust more than others, methods we think are more reliable, and so on. But when it comes to deciding the rules with which we'll govern a whole society, it seems to me like a core principle - almost a meta-principle - that before anything else we will ensure we've structured affairs so that people can freely exercise their reason and their conscience and come to their own understanding of the world. And given that we're social creatures who make up our minds collectively in groups both large and small, that requires at the very least certain kinds of robust protections for free speech and association.</p>
    </div>
  </body>
</post>