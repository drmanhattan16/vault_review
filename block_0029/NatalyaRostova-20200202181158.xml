<post>
  <author>NatalyaRostova</author>
  <date>2020-02-02T18:11:58Z</date>
  <link>/r/TheMotte/comments/exbynl/meta_just_keep_swimming/fgbxpl3/</link>
  <title>Sanity Checks Against Solving Problems with Terrorism</title>
  <body>
    <div class="md">
      <p>Okay, I have a few thoughts on this. Forgive the wall-of-text, but it's related to some things that have been swimming around in my head for a while, and this is a good chance to get them out.</p>
      <p>I don't think that's what it comes down to. That's a simple moral framework that is hard to disagree with, since it removes all uncertainty and ambiguity from the way in which we face those choices in reality.</p>
      <p>The problem is, in reality, predicting the future is harder than we can possibly imagine, and we never get to observe the counter-factual. I've spent the past 7 years of my life working on forecasting problems (first for the federal reserve, and then in industry, and it's hilariously difficult). Was fire-bombing Tokyo during WWII correct? I struggle with that question, and I think it probably was correct, as we were engaged in total war at the time with Japan, and as a result I would weigh the lives of my fellow countrymen over the Japanese citizens.</p>
      <p>But if we could have known that war would have ended the same way, while also avoiding incinerating a few million Japanese women and children, then it wouldn't have been defensible. I mean, I just don't get enjoyment out of burning women and children, and I'm sure you don't either.</p>
      <p>Yet even in this almost obvious case of total war, it's still so hard to know if it was the correct move.</p>
      <p>I'd suggest we categorize the case of WW2 as a first-order case in your moral framework. That is to say, there is a clear linear case for how killing will improve the future, that doesn't require more complex or nonlinear paths that the future will take. Put more simply: Kill the Japanese until none are left to kill us. This doesn't require a detailed manifesto or philosophical argument to make sense, which we can think of as a nonlinear path to the future.</p>
      <p>Second order cases require on more non-linear conditions being met, where perhaps injecting violent entropy can 'shock' a system to our preferred outcome. "If we just kill more people, they will reconsider and see that our economic/religious scriptures are clearly correct." Terrorism often lives in this region, although not always. What would be a case of justifiable (or first order) terrorism under this framework? Well, probably something like Braveheart with Mel Gibson, where they want the occupying soldiers out of their town. At least justifiable from their perspective. Moldbug enumerates on this in his post on Anders Brevik.</p>
      <p>The problem that arises with building a moral justification for second order terrorism though, is that it begins to try and introduce a violent shock into the system, based on an extremely challenging prediction. Predicting complicated non-linear futures that might arise from a shock is nearly impossible. To borrow some heuristics from the forecasting literature, it's pretty much unheard of for non-linear models to work well in long-run forecasting. The best we can do is extrapolate, with the correct conditions or extrapolation algorithms (I can provide citations on this if there is any interest).</p>
      <p>Here is where perhaps our consideration of uncertainty is more pivotal in how we view the world than any sort of moral framework: I don't believe you can forecast the future well enough to claim these killings or injuries are justified.</p>
      <p>But let's take it a step further, and ask what would happen if other people adopted both your moral framework, and your conception of uncertainty. Well, I know  of a lot of anti-semites that believe Jews are the ills of the US. I know of those who view abortion as murder. I know those who view conservative social structure as contributing to the deaths of sexual minorities. Well, we could go on, you get the point. All of these rely on harder to parameterize concepts of the second-order effects and harm these people cause.</p>
      <p>So, you think the environment is the absolute biggest problem. That's fine, but lots of other people think their pet issue is the absolute biggest problem. Why? Well, to me, it seems like a case of over-fitting. To the extent our brains our models, it's difficult for us to hold in our head all these absolute worst problems. We tend to become interested in some problem space, and then focus on it to the exclusion of others.</p>
      <p>Is it surprising that my friends who work in cyber view China as our existential threat? That my friends who grew up reading pol view immigration as our existential threat? That my friends who studied the sciences view global warming as our existential threat? The list goes on.</p>
      <p>For me, there are multiple steps to avoiding saying something crazy like "We should mail bombs to people in order to shock people in to seeing that I'm correct"</p>
      <ol>
        <li>The first step is to debase my own model of the world by accepting that I'm not particularly smarter than other people, and given that so many people have radical or eccentric views depending on what information they are exposed to, I'm probably the same.</li>
        <li>The second step is accept that, historically, it is extremely difficult to predict second-order effects. History and reality is impossibly complex and hard to map into the future. The history books tend to show that people who had some idea of a brighter future, and as a result decided to start killing people, usually just fuck things up worse. Rousseau sort of pointed this out in terms of revolutions, but no matter how many times this happens and things get worse, people often think 'this time it's different.'</li>
        <li>The third step is by realizing that injecting entropy/violence into a system is just as likely to make things worse for you. By what metric or confidence do you have that committing terrorist acts against civilians will further your mission? Why might it not just make it worse, by turning people against you? Again, you don't know. You have no idea. It's a prediction of a complex system. Where is your track record of predicting higher order effects in complex systems?</li>
        <li>Consider what it might look like if other people adopted your own moral framework. In this case we would probably see a growth in terrorism. Perhaps one day your parents would be severely injured or killed by a local separatist movement, or an anti-vaccine group. And why wouldn't they? I don't really see any reason why their pet existential threat, at least in their mind, is less clear than in your mind. And your preferred world would have already knocked down the shelling fence (in the Western world) that people at least implicitly hold, namely "don't kill random people even if you really really think that you're correct and this is a Really Big Deal."</li>
      </ol>
      <p>I guess this all comes back to what you said in the beginning 'conditional on some sanity checks.' I'm not convinced your checks are sane. And if you can't convince other people who don't intimately share your world-view that they are sane, then perhaps you could use some more robust sanity checks.</p>
      <p>This path happens so often, really smart people keep getting more interested and fascinated in their specific arguments and view of the world, and eventually well, you know, a *little* violence might be necessary, just here and there. And it creeps up on you. Always so justifiable. So clear.</p>
    </div>
  </body>
</post>